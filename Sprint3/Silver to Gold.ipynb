{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.10 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: d37cb8b1-5333-489d-9893-751e095dd1ec\nApplying the following default arguments:\n--glue_kernel_version 1.0.10\n--enable-glue-datacatalog true\nWaiting for session d37cb8b1-5333-489d-9893-751e095dd1ec to get into ready status...\nSession d37cb8b1-5333-489d-9893-751e095dd1ec has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, IntegerType\n\nspark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n\nSILVER_PATH = \"s3://btc-imat-bucket/btc_silver/\"\nGOLD_PATH   = \"s3://btc-imat-bucket/btc_gold/\"\n\nschema = StructType([\n    StructField(\"date\", TimestampType()),\n    StructField(\"sma_200\", DoubleType()),\n    StructField(\"ema_50\", DoubleType()),\n    StructField(\"rsi_14\", DoubleType()),\n    StructField(\"macd_12_26\", DoubleType()),\n    StructField(\"year\", IntegerType()),\n    StructField(\"month\", IntegerType()),\n])\n\ndef compute(pdf: pd.DataFrame) -> pd.DataFrame:\n    pdf = pdf.sort_values(\"date\").copy()\n    price = pd.to_numeric(pdf[\"close\"], errors=\"coerce\")\n\n    pdf[\"sma_200\"] = price.rolling(200, min_periods=200).mean()\n    pdf[\"ema_50\"]  = price.ewm(span=50, adjust=False, min_periods=50).mean()\n\n    delta = price.diff()\n    gain = delta.clip(lower=0)\n    loss = (-delta).clip(lower=0)\n\n    avg_gain = gain.ewm(alpha=1/14, adjust=False, min_periods=14).mean()\n    avg_loss = loss.ewm(alpha=1/14, adjust=False, min_periods=14).mean()\n\n    rs = avg_gain / avg_loss\n    pdf[\"rsi_14\"] = 100 - (100/(1+rs))\n\n    ema12 = price.ewm(span=12, adjust=False, min_periods=26).mean()\n    ema26 = price.ewm(span=26, adjust=False, min_periods=26).mean()\n    pdf[\"macd_12_26\"] = ema12 - ema26\n\n    dt = pd.to_datetime(pdf[\"date\"], utc=True, errors=\"coerce\")\n    pdf[\"year\"] = dt.dt.year.astype(\"int32\")\n    pdf[\"month\"] = dt.dt.month.astype(\"int32\")\n\n    return pdf[[\"date\", \"sma_200\", \"ema_50\", \"rsi_14\", \"macd_12_26\", \"year\", \"month\"]]\n\n# Leer Silver (todas particiones)\ndf = spark.read.parquet(SILVER_PATH)\n\n# Asegurar date como timestamp\ndf = (\n    df.withColumn(\"date\", F.to_timestamp(F.col(\"date\")))\n      .select(\"date\", \"close\")\n      .where(F.col(\"date\").isNotNull())\n      .orderBy(\"date\")\n)\n\n# Calcular KPIs (BTC completo)\nkpi = (\n    df.groupBy(F.lit(1).alias(\"grp\"))\n      .applyInPandas(compute, schema=schema)\n      .drop(\"grp\")\n)\n\n# Escribir Gold particionado year/month\n(\n    kpi.write\n       .mode(\"overwrite\")\n       .partitionBy(\"year\", \"month\")\n       .parquet(GOLD_PATH)\n)\n\nprint(\"✅ GOLD escrito en:\", GOLD_PATH)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "✅ GOLD escrito en: s3://btc-imat-bucket/btc_gold/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Comprobación",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df_check = spark.read.parquet(\"s3://btc-imat-bucket/btc_gold/year=2026/month=1/\")\n\ndf_check.orderBy(F.col(\"date\")).show(5, truncate=False)  # head (20 filas)\ndf_check.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+------------------+-----------------+-----------------+-------------------+\n|date               |sma_200           |ema_50           |rsi_14           |macd_12_26         |\n+-------------------+------------------+-----------------+-----------------+-------------------+\n|2026-01-01 00:00:00|106899.97869999999|91640.76285166692|49.56008699758408|-795.8375275088911 |\n|2026-01-02 00:00:00|106815.59785      |91573.75528885645|53.80162885227875|-580.0317092254554 |\n|2026-01-03 00:00:00|106746.1944       |91537.07625792091|56.15338895533933|-347.99442471022485|\n|2026-01-04 00:00:00|106679.68534999999|91538.62267917891|59.1289561947065 |-87.40128812752664 |\n|2026-01-05 00:00:00|106626.06895      |91631.3676721523 |65.40166692802941|303.4015543589485  |\n+-------------------+------------------+-----------------+-----------------+-------------------+\nonly showing top 5 rows\n\nroot\n |-- date: timestamp (nullable = true)\n |-- sma_200: double (nullable = true)\n |-- ema_50: double (nullable = true)\n |-- rsi_14: double (nullable = true)\n |-- macd_12_26: double (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}